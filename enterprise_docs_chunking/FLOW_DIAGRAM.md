# ğŸ”„ Enterprise Document Chunking - System Flow

## ğŸ“Š High-Level System Flow

```mermaid
graph TB
    A[ğŸ“„ Input Documents] --> B{ğŸ” Document Classifier}
    B --> C1[ğŸ“– Technical Doc]
    B --> C2[ğŸ’» Code Doc]
    B --> C3[ğŸ“‹ Policy Doc]
    B --> C4[ğŸ†˜ Support Doc]
    B --> C5[ğŸ“š Tutorial]
    
    C1 --> D1[âœ‚ï¸ Semantic Chunking]
    C2 --> D2[âœ‚ï¸ Code-Aware Chunking]
    C3 --> D3[âœ‚ï¸ Hierarchical Chunking]
    C4 --> D1
    C5 --> D3
    
    D1 --> E[ğŸ§  Embedding Generation]
    D2 --> E
    D3 --> E
    
    E --> F[ğŸ’¾ Qdrant Vector Store]
    F --> G[ğŸ” Semantic Search API]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
    style G fill:#e8eaf6
```

## ğŸ” Document Classification Flow

```mermaid
graph LR
    A[ğŸ“„ Document + Filename] --> B[ğŸ” Pattern Analysis]
    A --> C[ğŸ“‚ Extension Analysis]
    
    B --> D[Headers Detection]
    B --> E[Code Patterns]
    B --> F[Policy Terms]
    B --> G[Tutorial Markers]
    
    C --> H[File Extension Scoring]
    
    D --> I[ğŸ“Š Content Score 70%]
    E --> I
    F --> I
    G --> I
    
    H --> J[ğŸ“‚ Extension Score 30%]
    
    I --> K[ğŸ¯ Combined Scoring]
    J --> K
    
    K --> L{Confidence > 0.7?}
    L -->|Yes| M[âœ… Document Type]
    L -->|No| N[â“ Unknown Type]
    
    M --> O[âš™ï¸ Strategy Selection]
    N --> P[ğŸ”„ Fallback Strategy]
```

## âœ‚ï¸ Chunking Strategy Details

### Semantic Chunking Flow
```mermaid
graph TB
    A[ğŸ“„ Document Content] --> B[ğŸ”¤ Sentence Splitting]
    B --> C{ğŸ§  Embedder Available?}
    
    C -->|Yes| D[ğŸ”¢ Generate Embeddings]
    C -->|No| E[ğŸ“ Size-Based Boundaries]
    
    D --> F[ğŸ“Š Similarity Analysis]
    F --> G[ğŸ¯ Boundary Detection]
    G --> H[âœ‚ï¸ Smart Split]
    
    E --> I[ğŸ“ Size-Based Split]
    
    H --> J[ğŸ“ Semantic Chunks]
    I --> J
    
    J --> K[ğŸ”— Add Overlap]
    K --> L[âœ… Final Chunks]
```

### Code-Aware Chunking Flow
```mermaid
graph TB
    A[ğŸ’» Code Content] --> B[ğŸ” Language Detection]
    B --> C{Language Type}
    
    C -->|Python| D[ğŸ Python AST Parser]
    C -->|JavaScript| E[âš¡ JS Pattern Matcher]
    C -->|Other| F[ğŸ”§ Generic Code Parser]
    
    D --> G[ğŸ“¥ Extract Imports]
    D --> H[ğŸ”§ Extract Functions]
    D --> I[ğŸ—ï¸ Extract Classes]
    
    E --> J[ğŸ“¥ Extract Requires/Imports]
    E --> K[ğŸ”§ Extract Functions]
    E --> L[ğŸ—ï¸ Extract Classes]
    
    F --> M[ğŸ“ Generic Parsing]
    
    G --> N[ğŸ“¦ Group Imports]
    H --> O[ğŸ”§ Preserve Functions]
    I --> P[ğŸ—ï¸ Preserve Classes]
    
    J --> N
    K --> O
    L --> P
    
    M --> Q[ğŸ“ Generic Chunks]
    
    N --> R[âœ… Code Chunks]
    O --> R
    P --> R
    Q --> R
```

### Hierarchical Chunking Flow
```mermaid
graph TB
    A[ğŸ“‹ Structured Document] --> B[ğŸ” Header Detection]
    B --> C[ğŸ“Š Parse Document Structure]
    C --> D[ğŸ—ï¸ Build Section Tree]
    
    D --> E[ğŸ“ Section 1]
    D --> F[ğŸ“ Section 2]
    D --> G[ğŸ“ Section 3]
    
    E --> H[ğŸ“ Subsection 1.1]
    E --> I[ğŸ“ Subsection 1.2]
    
    F --> J[ğŸ“ Subsection 2.1]
    
    H --> K[ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Parent-Child Links]
    I --> K
    J --> K
    
    K --> L[ğŸ”— Breadcrumb Metadata]
    L --> M[âœ… Hierarchical Chunks]
```

## ğŸ§  Embedding & Storage Pipeline

```mermaid
graph LR
    A[ğŸ“ Text Chunks] --> B[ğŸ¤– HuggingFace Model]
    B --> C[ğŸ”¢ 384D Vectors]
    
    C --> D[ğŸ“‹ Chunk Metadata]
    D --> E[ğŸ“¦ Combined Payload]
    
    E --> F[ğŸ—„ï¸ Qdrant Collection]
    F --> G[ğŸ“Š Vector Index]
    
    G --> H[ğŸ” Similarity Search]
    H --> I[ğŸ“ˆ Ranked Results]
    
    style B fill:#e8f5e8
    style F fill:#fff3e0
    style H fill:#e8eaf6
```

## ğŸ” Search & Retrieval Flow

```mermaid
graph TB
    A[â“ User Query] --> B[ğŸ§  Query Embedding]
    B --> C[ğŸ” Vector Search]
    
    C --> D{ğŸ·ï¸ Filters Applied?}
    D -->|Yes| E[ğŸ“‹ Metadata Filter]
    D -->|No| F[ğŸ” Direct Search]
    
    E --> F
    F --> G[ğŸ“Š Cosine Similarity]
    G --> H[ğŸ“ˆ Score Ranking]
    
    H --> I[ğŸ¯ Top-K Results]
    I --> J[ğŸ“ Chunk Content]
    I --> K[ğŸ“‹ Metadata]
    I --> L[ğŸ“Š Similarity Score]
    
    J --> M[âœ… Search Results]
    K --> M
    L --> M
```

## ğŸ“Š Data Transformation Pipeline

```mermaid
graph LR
    A[ğŸ“„ Raw Document] --> B[ğŸ” Classification]
    B --> C[ğŸ“Š Document Type]
    
    C --> D[âœ‚ï¸ Chunking Strategy]
    D --> E[ğŸ“ Text Chunks]
    
    E --> F[ğŸ§  Embedding Model]
    F --> G[ğŸ”¢ Vector Embeddings]
    
    G --> H[ğŸ“‹ Metadata Attachment]
    H --> I[ğŸ’¾ Vector Storage]
    
    I --> J[ğŸ” Searchable Index]
    
    subgraph "Metadata Flow"
        K[ğŸ“‚ File Info] --> H
        L[ğŸ·ï¸ Chunk Type] --> H
        M[ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Relationships] --> H
        N[ğŸ“Š Confidence] --> H
    end
```

## ğŸ¯ Quality Control Flow

```mermaid
graph TB
    A[ğŸ“ Generated Chunks] --> B[ğŸ” Quality Checks]
    
    B --> C{ğŸ“ Size Check}
    B --> D{ğŸ§  Coherence Check}
    B --> E{ğŸ—ï¸ Structure Check}
    
    C -->|Pass| F[âœ… Size Valid]
    C -->|Fail| G[âš ï¸ Size Warning]
    
    D -->|Pass| H[âœ… Coherent]
    D -->|Fail| I[âš ï¸ Split Required]
    
    E -->|Pass| J[âœ… Structure Preserved]
    E -->|Fail| K[âš ï¸ Structure Lost]
    
    F --> L[ğŸ“Š Quality Score]
    G --> L
    H --> L
    I --> L
    J --> L
    K --> L
    
    L --> M{Quality > Threshold?}
    M -->|Yes| N[âœ… Accept Chunks]
    M -->|No| O[ğŸ”„ Retry Processing]
```

## ğŸ“ˆ Performance Monitoring Flow

```mermaid
graph LR
    A[â±ï¸ Processing Start] --> B[ğŸ“Š Metrics Collection]
    
    B --> C[â±ï¸ Processing Time]
    B --> D[ğŸ“ Chunk Count]
    B --> E[ğŸ¯ Classification Confidence]
    B --> F[ğŸ’¾ Storage Success]
    
    C --> G[ğŸ“ˆ Performance Dashboard]
    D --> G
    E --> G
    F --> G
    
    G --> H[ğŸ“Š Analytics]
    H --> I[ğŸ”„ Optimization Insights]
    
    I --> J[âš™ï¸ Parameter Tuning]
    J --> K[ğŸ¯ Improved Performance]
```

## ğŸ”„ Complete System Integration

```mermaid
graph TB
    subgraph "Input Layer"
        A1[ğŸ“„ PDF Documents]
        A2[ğŸ“ Markdown Files]
        A3[ğŸ’» Code Files]
        A4[ğŸ“‹ Text Documents]
    end
    
    subgraph "Processing Layer"
        B[ğŸ” Document Loader]
        C[ğŸ¯ Document Classifier]
        D[âœ‚ï¸ Chunking Engine]
        E[ğŸ§  Embedding Generator]
    end
    
    subgraph "Storage Layer"
        F[ğŸ’¾ Qdrant Vector DB]
        G[ğŸ“Š Metadata Store]
    end
    
    subgraph "API Layer"
        H[ğŸ” Search API]
        I[ğŸ“ Processing API]
        J[ğŸ“Š Analytics API]
    end
    
    A1 --> B
    A2 --> B
    A3 --> B
    A4 --> B
    
    B --> C
    C --> D
    D --> E
    E --> F
    E --> G
    
    F --> H
    G --> H
    F --> I
    G --> I
    F --> J
    G --> J
    
    style B fill:#e1f5fe
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff3e0
    style F fill:#e8eaf6
    style H fill:#f1f8e9
```

---

## ğŸ“‹ Processing Steps Summary

1. **ğŸ“„ Document Input**: Multi-format document ingestion
2. **ğŸ” Classification**: Pattern-based document type detection
3. **âš™ï¸ Strategy Selection**: Mapping document type to chunking strategy
4. **âœ‚ï¸ Adaptive Chunking**: Content-aware chunk generation
5. **ğŸ§  Embedding**: Vector representation generation
6. **ğŸ’¾ Storage**: Persistent vector storage with metadata
7. **ğŸ” Search**: Semantic similarity search and ranking
8. **ğŸ“Š Analytics**: Performance monitoring and optimization

Each step is designed to preserve document structure and semantic meaning while optimizing for retrieval accuracy in enterprise knowledge management scenarios. 