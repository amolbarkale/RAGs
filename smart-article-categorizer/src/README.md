# Smart Article Categorizer

A demonstration pipeline that classifies news articles into six categoriesâ€”**Tech**, **Finance**, **Healthcare**, **Sports**, **Politics**, and **Entertainment**â€”using four different embedding methods and a simple Logistic Regression classifier.

---

## ğŸš€ Features

- **Four Embedding Backends**  
  1. **Word2Vec/GloVe** (average word vectors)  
  2. **BERT** ("[CLS]" token)  
  3. **Sentenceâ€‘BERT** (allâ€‘MiniLMâ€‘L6â€‘v2)  

- **Classification Pipeline**  
  - Train one Logistic Regression per embedding  
  - Evaluate Accuracy, Precision, Recall & Fâ‚ (weighted)  
  - Compare performance across embeddings

- **Commandâ€‘Line & Web UI**  
  - `main.py` for terminalâ€‘based inference  
  - `streamlit_app.py` for realâ€‘time browser UI  
  - Confidence tables & barâ€‘charts  
  - Optional UMAP cluster visualization

- **Flexible Data Loading**  
  - Load & split multiple CSVs  
  - Or pull from HuggingFace datasets (Education, PubMedQA, Sports)

---

## ğŸ“ Directory Structure

```
smart_article_categorizer/
â”œâ”€â”€ data/                    # (optional) your CSV files
â”œâ”€â”€ models/                  # pretrained Word2Vec + trained .joblib classifiers
â”œâ”€â”€ results/                 # performance report & sample data for UI clusters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py       # CSV + HF dataset loading & train/test split
â”‚   â”œâ”€â”€ embedding_models.py  # Word2Vec, BERT, & Sentenceâ€‘BERT embedding funcs
â”‚   â”œâ”€â”€ train.py             # endâ€‘toâ€‘end training & eval pipeline
â”‚   â”œâ”€â”€ main.py              # CLI for singleâ€‘article classification
â”‚   â””â”€â”€ streamlit_app.py     # Streamlitâ€‘based web dashboard
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # this file
```

---

## âš™ï¸ Prerequisites

- Python 3.8+  
- (Optional) `git`, `virtualenv`  
- HuggingFace account & `huggingface-cli login` if you load private HF datasets  

---

## ğŸ›  Installation

1. **Clone the repo**  
   ```bash
   git clone https://github.com/yourâ€‘username/smart_article_categorizer.git
   cd smart_article_categorizer
   ```

2. **Create & activate a virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate      # macOS/Linux
   venv\Scripts\activate         # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **(Optional) Download pretrained Word2Vec**
   Place `GoogleNewsâ€‘vectorsâ€‘negative300.bin` into `models/` or update its path in `train.py` & `streamlit_app.py`.

---

## ğŸ“Š Training & Evaluation

Run the full pipeline on your chosen data source:

```bash
cd src
python train.py
```

This will:
- Load & split your data (CSV or HuggingFace datasets)
- Compute embeddings for each method
- Train & evaluate one LogisticRegression per embedding
- Serialize `.joblib` models into `models/`
- Write `results/embedding_performance.csv` with metrics

---

## ğŸ’» CLI Usage

Classify a single article from the terminal:

```bash
python src/main.py --text "Your article text here."
# or
python src/main.py --file /path/to/article.txt
```

---

## ğŸŒ Web Dashboard

1. **Run Streamlit**
   ```bash
   cd src
   streamlit run streamlit_app.py
   ```

2. **In your browser**
   - Paste an article into the textarea
   - Click **Classify**
   - View prediction table, confidence barâ€‘chart
   - (Optional) enable UMAP clustering

---

## ğŸ§¹ Clearing HuggingFace Cache

If you've pulled HF datasets and want to reclaim space:

**Manual (Linux/macOS):**
```bash
rm -rf ~/.cache/huggingface/datasets
```

**Programmatic:** Set `cache_dir="../hf_cache"` in your `load_dataset` calls, then after training:
```python
import shutil
shutil.rmtree("../hf_cache", ignore_errors=True)
```

---

## ğŸ“ˆ Analysis & Recommendations

After training, inspect `results/embedding_performance.csv` to see which embedding approach:
- Maximizes Fâ‚â€‘score
- Balances precision vs. recall
- Offers the fastest runtime for your data size

Use that insight to choose your production embedding.

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/foo`)
3. Commit your changes (`git commit -m "feat: add bar"`)
4. Push (`git push origin feature/foo`) & open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- HuggingFace for transformers and datasets
- Streamlit for the web interface
- Google for Word2Vec pretrained vectors